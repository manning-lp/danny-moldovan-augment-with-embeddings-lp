{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: Tiktoken in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: OpenAI in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.31.1)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: SciPy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (5.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from BeautifulSoup4) (2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Tiktoken) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (2.7.2)\n",
      "Requirement already satisfied: sniffio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from OpenAI) (4.12.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.5.0->OpenAI) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.5.0->OpenAI) (1.2.1)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1,>=0.23.0->OpenAI) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1,>=0.23.0->OpenAI) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->OpenAI) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->OpenAI) (2.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->Tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->Tiktoken) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4 Tiktoken OpenAI pandas SciPy lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the subdirectory path\n",
    "subdirectory_path = 'apidocs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_content_from_files_in_directory(subdirectory_path):\n",
    "    # Use glob to recursively find all files\n",
    "    all_files = glob.glob(os.path.join(subdirectory_path, '**'), recursive=True)\n",
    "    \n",
    "    # Define a regex pattern to match files ending with .html\n",
    "    pattern = re.compile(r'.*\\.html$')\n",
    "    \n",
    "    # Filter the files using the regex pattern\n",
    "    html_files = [file for file in all_files if pattern.match(file)]\n",
    "    \n",
    "    html_texts = {}\n",
    "    \n",
    "    for f in html_files:\n",
    "        # Load the HTML file\n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        \n",
    "        # Extract the text content\n",
    "        text_content = soup.get_text()\n",
    "        \n",
    "        html_texts[f] = text_content\n",
    "\n",
    "    return html_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import itertools\n",
    "\n",
    "# Select the encoding based on the model\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "max_chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify_text(text, max_tokens, encoding):\n",
    "    # Tokenize the text\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    # Initialize variables for chunking\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            # If current chunk has reached the max token limit, save it\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "\n",
    "        current_chunk.append(token)\n",
    "\n",
    "    # Add the last chunk if any tokens left\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # Decode tokens back to text chunks\n",
    "    text_chunks = [encoding.decode(chunk) for chunk in chunks]\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Hello, world! This is a sample text to tokenize and chunkify using tiktoken.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Hello, world! This is a sample text to tokenize and chunkify using tiktoken.\"\n",
    "text_chunks = chunkify_text(text, max_chunk_size, encoding)\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, world! This is a sample text to tokenize and chunkify using tiktoken.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_and_chunkify_files_in_directory(subdirectory_path):\n",
    "    html_texts = extract_html_content_from_files_in_directory(subdirectory_path)\n",
    "\n",
    "    text_chunks = {}\n",
    "    \n",
    "    for f in html_texts.keys():\n",
    "        text_chunks[f] =  chunkify_text(html_texts[f], max_chunk_size, encoding)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = extract_html_and_chunkify_files_in_directory(subdirectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n\\n\\n\\nOverview: module code â€” ðŸ¦œðŸ”— LangChain 0.0.305\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAPI\\n\\n\\nExperimental\\n\\n\\nPython Docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle Menu\\n\\n\\n\\nPrev\\nUp\\nNext\\n\\n\\n\\nLangChain 0.0.305\\n\\n\\n\\n\\n\\n\\n\\n\\nAll modules for which code is available\\nlangchain.adapters.openai\\nlangchain.agents.agent\\nlangchain.agents.agent_iterator\\nlangchain.agents.agent_toolkits.ainetwork.toolkit\\nlangchain.agents.agent_toolkits.amadeus.toolkit\\nlangchain.agents.agent_toolkits.azure_cognitive_services\\nlangchain.agents.agent_toolkits.base\\nlangchain.agents.agent_toolkits.conversational_retrieval.openai_functions\\nlangchain.agents.agent_toolkits.conversational_retrieval.tool\\nlangchain.agents.agent_toolkits.csv.base\\nlangchain.agents.agent_toolkits.file_management.toolkit\\nlangchain.agents.agent_toolkits.github.toolkit\\nlangchain.agents.agent_toolkits.gitlab.toolkit\\nlangchain.agents.agent_toolkits.gmail.toolkit\\nlangchain.agents.agent_toolkits.jira.toolkit\\nlangchain.agents.agent_toolkits.json.base\\nlangchain.agents.agent_toolkits.json.toolkit\\nlangchain.agents.agent_toolkits.multion.toolkit\\nlangchain.agents.agent_toolkits.nla.tool\\nlangchain.agents.agent_toolkits.nla.toolkit\\nlangchain.agents.agent_toolkits.office365.toolkit\\nlangchain.agents.agent_toolkits.openapi.base\\nlangchain.agents.agent_toolkits.openapi.planner\\nlangchain.agents.agent_toolkits.openapi.spec\\nlangchain.agents.agent_toolkits.openapi.toolkit\\nlangchain.agents.agent_toolkits.pandas.base\\nlangchain.agents.agent_toolkits.playwright.toolkit\\nlangchain.agents.agent_toolkits.powerbi.base\\nlangchain.agents.agent_toolkits.powerbi.chat_base\\nlangchain.agents.agent_toolkits.powerbi.toolkit\\nlangchain.agents.agent_toolkits.python.base\\nlangchain.agents.agent_toolkits.spark.base\\nlangchain.agents.agent_toolkits.spark_sql.base\\nlangchain.agents.agent_toolkits.spark_sql.toolkit\\nlangchain.agents.agent_toolkits.sql.base\\nlangchain.agents.agent_toolkits.sql.toolkit\\nlangchain.agents.agent_toolkits.vectorstore.base\\nlangchain.agents.agent_toolkits.vectorstore.toolkit\\nlangchain.agents.agent_toolkits.xorbits.base\\nlangchain.agents.agent_toolkits.zapier.toolkit\\nlangchain.agents.agent_types\\nlangchain.agents.chat',\n",
       " '.base\\nlangchain.agents.chat.output_parser\\nlangchain.agents.conversational.base\\nlangchain.agents.conversational.output_parser\\nlangchain.agents.conversational_chat.base\\nlangchain.agents.conversational_chat.output_parser\\nlangchain.agents.format_scratchpad.log\\nlangchain.agents.format_scratchpad.log_to_messages\\nlangchain.agents.format_scratchpad.openai_functions\\nlangchain.agents.format_scratchpad.xml\\nlangchain.agents.initialize\\nlangchain.agents.load_tools\\nlangchain.agents.loading\\nlangchain.agents.mrkl.base\\nlangchain.agents.mrkl.output_parser\\nlangchain.agents.openai_functions_agent.agent_token_buffer_memory\\nlangchain.agents.openai_functions_agent.base\\nlangchain.agents.openai_functions_multi_agent.base\\nlangchain.agents.output_parsers.json\\nlangchain.agents.output_parsers.openai_functions\\nlangchain.agents.output_parsers.react_json_single_input\\nlangchain.agents.output_parsers.react_single_input\\nlangchain.agents.output_parsers.self_ask\\nlangchain.agents.output_parsers.xml\\nlangchain.agents.react.base\\nlangchain.agents.react.output_parser\\nlangchain.agents.schema\\nlangchain.agents.self_ask_with_search.base\\nlangchain.agents.structured_chat.base\\nlangchain.agents.structured_chat.output_parser\\nlangchain.agents.tools\\nlangchain.agents.utils\\nlangchain.agents.xml.base\\nlangchain.cache\\nlangchain.callbacks.aim_callback\\nlangchain.callbacks.argilla_callback\\nlangchain.callbacks.arize_callback\\nlangchain.callbacks.arthur_callback\\nlangchain.callbacks.base\\nlangchain.callbacks.clearml_callback\\nlangchain.callbacks.comet_ml_callback\\nlangchain.callbacks.confident_callback\\nlangchain.callbacks.context_callback\\nlangchain.callbacks.file\\nlangchain.callbacks.flyte_callback\\nlangchain.callbacks.human\\nlangchain.callbacks.infino_callback\\nlangchain.callbacks.labelstudio_callback\\nlangchain.callbacks.llmonitor_callback\\nlangchain.callbacks.manager\\nlangchain.callbacks.mlflow_callback\\nlangchain.callbacks.openai_info\\nlangchain.callbacks.promptlayer_callback\\nlangchain.callbacks.sagemaker_callback\\nlangchain.callbacks.stdout\\nlangchain.callbacks.streaming_aiter\\nlangchain.callbacks.streaming_aiter_final_only\\nlangchain.callbacks.streaming_stdout\\nlangchain.callbacks.streaming_stdout_final_only\\nlangchain.callbacks.streamlit.mutable_expander\\nlangchain.callbacks.streamlit.streamlit_callback_handler\\nlangchain.callbacks.tracers.base\\nlangchain.callbacks.tracers.evaluation\\nlangchain.callbacks.tracers',\n",
       " '.langchain\\nlangchain.callbacks.tracers.langchain_v1\\nlangchain.callbacks.tracers.log_stream\\nlangchain.callbacks.tracers.run_collector\\nlangchain.callbacks.tracers.schemas\\nlangchain.callbacks.tracers.stdout\\nlangchain.callbacks.tracers.wandb\\nlangchain.callbacks.trubrics_callback\\nlangchain.callbacks.utils\\nlangchain.callbacks.wandb_callback\\nlangchain.callbacks.whylabs_callback\\nlangchain.chains.api.base\\nlangchain.chains.api.openapi.chain\\nlangchain.chains.api.openapi.requests_chain\\nlangchain.chains.api.openapi.response_chain\\nlangchain.chains.base\\nlangchain.chains.combine_documents.base\\nlangchain.chains.combine_documents.map_reduce\\nlangchain.chains.combine_documents.map_rerank\\nlangchain.chains.combine_documents.reduce\\nlangchain.chains.combine_documents.refine\\nlangchain.chains.combine_documents.stuff\\nlangchain.chains.constitutional_ai.base\\nlangchain.chains.constitutional_ai.models\\nlangchain.chains.conversation.base\\nlangchain.chains.conversational_retrieval.base\\nlangchain.chains.elasticsearch_database.base\\nlangchain.chains.example_generator\\nlangchain.chains.flare.base\\nlangchain.chains.flare.prompts\\nlangchain.chains.graph_qa.arangodb\\nlangchain.chains.graph_qa.base\\nlangchain.chains.graph_qa.cypher\\nlangchain.chains.graph_qa.falkordb\\nlangchain.chains.graph_qa.hugegraph\\nlangchain.chains.graph_qa.kuzu\\nlangchain.chains.graph_qa.nebulagraph\\nlangchain.chains.graph_qa.neptune_cypher\\nlangchain.chains.graph_qa.sparql\\nlangchain.chains.hyde.base\\nlangchain.chains.llm\\nlangchain.chains.llm_bash.base\\nlangchain.chains.llm_bash.prompt\\nlangchain.chains.llm_checker.base\\nlangchain.chains.llm_math.base\\nlangchain.chains.llm_requests\\nlangchain.chains.llm_summarization_checker.base\\nlangchain.chains.llm_symbolic_math.base\\nlangchain.chains.loading\\nlangchain.chains.mapreduce\\nlangchain.chains.moderation\\nlangchain.chains.natbot.base\\nlangchain.chains.natbot.crawler\\nlangchain.chains.openai_functions.base\\nlangchain.chains.openai_functions.citation_fuzzy_match\\nlangchain.chains.openai_functions.extraction\\nlangchain.chains.openai_functions.openapi\\nlang',\n",
       " 'chain.chains.openai_functions.qa_with_structure\\nlangchain.chains.openai_functions.tagging\\nlangchain.chains.openai_functions.utils\\nlangchain.chains.prompt_selector\\nlangchain.chains.qa_generation.base\\nlangchain.chains.qa_with_sources.base\\nlangchain.chains.qa_with_sources.loading\\nlangchain.chains.qa_with_sources.retrieval\\nlangchain.chains.qa_with_sources.vector_db\\nlangchain.chains.query_constructor.base\\nlangchain.chains.query_constructor.ir\\nlangchain.chains.query_constructor.parser\\nlangchain.chains.query_constructor.schema\\nlangchain.chains.retrieval_qa.base\\nlangchain.chains.router.base\\nlangchain.chains.router.embedding_router\\nlangchain.chains.router.llm_router\\nlangchain.chains.router.multi_prompt\\nlangchain.chains.router.multi_retrieval_qa\\nlangchain.chains.sequential\\nlangchain.chains.sql_database.query\\nlangchain.chains.transform\\nlangchain.chat_loaders.base\\nlangchain.chat_loaders.facebook_messenger\\nlangchain.chat_loaders.gmail\\nlangchain.chat_loaders.imessage\\nlangchain.chat_loaders.slack\\nlangchain.chat_loaders.telegram\\nlangchain.chat_loaders.utils\\nlangchain.chat_loaders.whatsapp\\nlangchain.chat_models.anthropic\\nlangchain.chat_models.anyscale\\nlangchain.chat_models.azure_openai\\nlangchain.chat_models.azureml_endpoint\\nlangchain.chat_models.baidu_qianfan_endpoint\\nlangchain.chat_models.base\\nlangchain.chat_models.bedrock\\nlangchain.chat_models.ernie\\nlangchain.chat_models.fake\\nlangchain.chat_models.fireworks\\nlangchain.chat_models.google_palm\\nlangchain.chat_models.human\\nlangchain.chat_models.javelin_ai_gateway\\nlangchain.chat_models.jinachat\\nlangchain.chat_models.konko\\nlangchain.chat_models.litellm\\nlangchain.chat_models.minimax\\nlangchain.chat_models.mlflow_ai_gateway\\nlangchain.chat_models.ollama\\nlangchain.chat_models.openai\\nlangchain.chat_models.promptlayer_openai\\nlangchain.chat_models.vertexai\\nlangchain.docstore.arbitrary_fn\\nlangchain.docstore.base\\nlangchain.docstore.in_memory\\nlangchain.docstore.wikipedia\\nlangchain.document_loaders.acreom\\nlangchain.document_loaders.airbyte\\nlangchain.document_loaders.airbyte_json\\nlangchain.document_loaders.airtable\\nlangchain.document_loaders.apify_dataset\\nlangchain.document_loaders.arcgis_loader\\nlangchain.document',\n",
       " '_loaders.arxiv\\nlangchain.document_loaders.assemblyai\\nlangchain.document_loaders.async_html\\nlangchain.document_loaders.azlyrics\\nlangchain.document_loaders.azure_blob_storage_container\\nlangchain.document_loaders.azure_blob_storage_file\\nlangchain.document_loaders.base\\nlangchain.document_loaders.base_o365\\nlangchain.document_loaders.bibtex\\nlangchain.document_loaders.bigquery\\nlangchain.document_loaders.bilibili\\nlangchain.document_loaders.blackboard\\nlangchain.document_loaders.blob_loaders.file_system\\nlangchain.document_loaders.blob_loaders.schema\\nlangchain.document_loaders.blob_loaders.youtube_audio\\nlangchain.document_loaders.blockchain\\nlangchain.document_loaders.brave_search\\nlangchain.document_loaders.browserless\\nlangchain.document_loaders.chatgpt\\nlangchain.document_loaders.chromium\\nlangchain.document_loaders.college_confidential\\nlangchain.document_loaders.concurrent\\nlangchain.document_loaders.confluence\\nlangchain.document_loaders.conllu\\nlangchain.document_loaders.csv_loader\\nlangchain.document_loaders.cube_semantic\\nlangchain.document_loaders.datadog_logs\\nlangchain.document_loaders.dataframe\\nlangchain.document_loaders.diffbot\\nlangchain.document_loaders.directory\\nlangchain.document_loaders.discord\\nlangchain.document_loaders.docugami\\nlangchain.document_loaders.dropbox\\nlangchain.document_loaders.duckdb_loader\\nlangchain.document_loaders.email\\nlangchain.document_loaders.embaas\\nlangchain.document_loaders.epub\\nlangchain.document_loaders.etherscan\\nlangchain.document_loaders.evernote\\nlangchain.document_loaders.excel\\nlangchain.document_loaders.facebook_chat\\nlangchain.document_loaders.fauna\\nlangchain.document_loaders.figma\\nlangchain.document_loaders.gcs_directory\\nlangchain.document_loaders.gcs_file\\nlangchain.document_loaders.generic\\nlangchain.document_loaders.geodataframe\\nlangchain.document_loaders.git\\nlangchain.document_loaders.gitbook\\nlangchain.document_loaders.github\\nlangchain.document_loaders.googledrive\\nlangchain.document_loaders.gutenberg\\nlangchain.document_loaders.helpers\\nlangchain.document_loaders.hn\\nlangchain.document_loaders.html\\nlangchain.document_loaders.html_bs\\nlangchain.document_loaders.hugging_face_dataset\\nlangchain.document_loaders.ifixit\\nlangchain.document_loaders.image\\nlangchain.document_loaders.image_captions\\nlangchain.document',\n",
       " '_loaders.imsdb\\nlangchain.document_loaders.iugu\\nlangchain.document_loaders.joplin\\nlangchain.document_loaders.json_loader\\nlangchain.document_loaders.larksuite\\nlangchain.document_loaders.markdown\\nlangchain.document_loaders.mastodon\\nlangchain.document_loaders.max_compute\\nlangchain.document_loaders.mediawikidump\\nlangchain.document_loaders.merge\\nlangchain.document_loaders.mhtml\\nlangchain.document_loaders.modern_treasury\\nlangchain.document_loaders.mongodb\\nlangchain.document_loaders.news\\nlangchain.document_loaders.notebook\\nlangchain.document_loaders.notion\\nlangchain.document_loaders.notiondb\\nlangchain.document_loaders.nuclia\\nlangchain.document_loaders.obs_directory\\nlangchain.document_loaders.obs_file\\nlangchain.document_loaders.obsidian\\nlangchain.document_loaders.odt\\nlangchain.document_loaders.onedrive\\nlangchain.document_loaders.onedrive_file\\nlangchain.document_loaders.open_city_data\\nlangchain.document_loaders.org_mode\\nlangchain.document_loaders.parsers.audio\\nlangchain.document_loaders.parsers.docai\\nlangchain.document_loaders.parsers.generic\\nlangchain.document_loaders.parsers.grobid\\nlangchain.document_loaders.parsers.html.bs4\\nlangchain.document_loaders.parsers.language.code_segmenter\\nlangchain.document_loaders.parsers.language.javascript\\nlangchain.document_loaders.parsers.language.language_parser\\nlangchain.document_loaders.parsers.language.python\\nlangchain.document_loaders.parsers.msword\\nlangchain.document_loaders.parsers.pdf\\nlangchain.document_loaders.parsers.registry\\nlangchain.document_loaders.parsers.txt\\nlangchain.document_loaders.pdf\\nlangchain.document_loaders.polars_dataframe\\nlangchain.document_loaders.powerpoint\\nlangchain.document_loaders.psychic\\nlangchain.document_loaders.pubmed\\nlangchain.document_loaders.pyspark_dataframe\\nlangchain.document_loaders.python\\nlangchain.document_loaders.readthedocs\\nlangchain.document_loaders.recursive_url_loader\\nlangchain.document_loaders.reddit\\nlangchain.document_loaders.roam\\nlangchain.document_loaders.rocksetdb\\nlangchain.document_loaders.rss\\nlangchain.document_loaders.rst\\nlangchain.document_loaders.rtf\\nlangchain.document_loaders.s3_directory\\nlangchain.document_loaders.s3_file\\nlangchain.document_loaders.sharepoint\\nlangchain.document_loaders.sitemap\\nlangchain.document_loaders.slack_directory\\nlangchain.document_loaders',\n",
       " '.snowflake_loader\\nlangchain.document_loaders.spreedly\\nlangchain.document_loaders.srt\\nlangchain.document_loaders.stripe\\nlangchain.document_loaders.telegram\\nlangchain.document_loaders.tencent_cos_directory\\nlangchain.document_loaders.tencent_cos_file\\nlangchain.document_loaders.tensorflow_datasets\\nlangchain.document_loaders.text\\nlangchain.document_loaders.tomarkdown\\nlangchain.document_loaders.toml\\nlangchain.document_loaders.trello\\nlangchain.document_loaders.tsv\\nlangchain.document_loaders.twitter\\nlangchain.document_loaders.unstructured\\nlangchain.document_loaders.url\\nlangchain.document_loaders.url_playwright\\nlangchain.document_loaders.url_selenium\\nlangchain.document_loaders.weather\\nlangchain.document_loaders.web_base\\nlangchain.document_loaders.whatsapp_chat\\nlangchain.document_loaders.wikipedia\\nlangchain.document_loaders.word_document\\nlangchain.document_loaders.xml\\nlangchain.document_loaders.xorbits\\nlangchain.document_loaders.youtube\\nlangchain.document_transformers.beautiful_soup_transformer\\nlangchain.document_transformers.doctran_text_extract\\nlangchain.document_transformers.doctran_text_qa\\nlangchain.document_transformers.doctran_text_translate\\nlangchain.document_transformers.embeddings_redundant_filter\\nlangchain.document_transformers.html2text\\nlangchain.document_transformers.long_context_reorder\\nlangchain.document_transformers.nuclia_text_transform\\nlangchain.document_transformers.openai_functions\\nlangchain.embeddings.aleph_alpha\\nlangchain.embeddings.awa\\nlangchain.embeddings.baidu_qianfan_endpoint\\nlangchain.embeddings.bedrock\\nlangchain.embeddings.cache\\nlangchain.embeddings.clarifai\\nlangchain.embeddings.cohere\\nlangchain.embeddings.dashscope\\nlangchain.embeddings.deepinfra\\nlangchain.embeddings.edenai\\nlangchain.embeddings.elasticsearch\\nlangchain.embeddings.embaas\\nlangchain.embeddings.ernie\\nlangchain.embeddings.fake\\nlangchain.embeddings.google_palm\\nlangchain.embeddings.gpt4all\\nlangchain.embeddings.gradient_ai\\nlangchain.embeddings.huggingface\\nlangchain.embeddings.huggingface_hub\\nlangchain.embeddings.javelin_ai_gateway\\nlangchain.embeddings.jina\\nlangchain.embeddings.llamacpp\\nlangchain.embeddings.llm_rails\\nlangchain.embeddings.localai\\nlangchain.embeddings.minimax\\nlangchain.embeddings.mlflow_gateway\\nlangchain.embeddings.model',\n",
       " 'scope_hub\\nlangchain.embeddings.mosaicml\\nlangchain.embeddings.nlpcloud\\nlangchain.embeddings.octoai_embeddings\\nlangchain.embeddings.ollama\\nlangchain.embeddings.openai\\nlangchain.embeddings.sagemaker_endpoint\\nlangchain.embeddings.self_hosted\\nlangchain.embeddings.self_hosted_hugging_face\\nlangchain.embeddings.spacy_embeddings\\nlangchain.embeddings.tensorflow_hub\\nlangchain.embeddings.vertexai\\nlangchain.embeddings.xinference\\nlangchain.evaluation.agents.trajectory_eval_chain\\nlangchain.evaluation.comparison.eval_chain\\nlangchain.evaluation.criteria.eval_chain\\nlangchain.evaluation.embedding_distance.base\\nlangchain.evaluation.exact_match.base\\nlangchain.evaluation.loading\\nlangchain.evaluation.parsing.base\\nlangchain.evaluation.qa.eval_chain\\nlangchain.evaluation.qa.generate_chain\\nlangchain.evaluation.regex_match.base\\nlangchain.evaluation.schema\\nlangchain.evaluation.string_distance.base\\nlangchain.graphs.arangodb_graph\\nlangchain.graphs.falkordb_graph\\nlangchain.graphs.graph_document\\nlangchain.graphs.hugegraph\\nlangchain.graphs.kuzu_graph\\nlangchain.graphs.memgraph_graph\\nlangchain.graphs.nebula_graph\\nlangchain.graphs.neo4j_graph\\nlangchain.graphs.neptune_graph\\nlangchain.graphs.networkx_graph\\nlangchain.graphs.rdf_graph\\nlangchain.hub\\nlangchain.indexes.base\\nlangchain.indexes.graph\\nlangchain.indexes.vectorstore\\nlangchain.llms.ai21\\nlangchain.llms.aleph_alpha\\nlangchain.llms.amazon_api_gateway\\nlangchain.llms.anthropic\\nlangchain.llms.anyscale\\nlangchain.llms.aviary\\nlangchain.llms.azureml_endpoint\\nlangchain.llms.baidu_qianfan_endpoint\\nlangchain.llms.bananadev\\nlangchain.llms.base\\nlangchain.llms.baseten\\nlangchain.llms.beam\\nlangchain.llms.bedrock\\nlangchain.llms.bittensor\\nlangchain.llms.cerebriumai\\nlangchain.llms.chatglm\\nlangchain.llms.clarifai\\nlangchain.llms.cohere\\nlangchain.llms.ctransformers\\nlangchain.llms.ctranslate2\\nlangchain.llms.databricks\\nlangchain.llms.deepinfra\\nlangchain.llms.deepsparse\\nlangchain.llms.edenai\\nlangchain',\n",
       " '.llms.fake\\nlangchain.llms.fireworks\\nlangchain.llms.forefrontai\\nlangchain.llms.google_palm\\nlangchain.llms.gooseai\\nlangchain.llms.gpt4all\\nlangchain.llms.gradient_ai\\nlangchain.llms.huggingface_endpoint\\nlangchain.llms.huggingface_hub\\nlangchain.llms.huggingface_pipeline\\nlangchain.llms.huggingface_text_gen_inference\\nlangchain.llms.human\\nlangchain.llms.javelin_ai_gateway\\nlangchain.llms.koboldai\\nlangchain.llms.llamacpp\\nlangchain.llms.loading\\nlangchain.llms.manifest\\nlangchain.llms.minimax\\nlangchain.llms.mlflow_ai_gateway\\nlangchain.llms.modal\\nlangchain.llms.mosaicml\\nlangchain.llms.nlpcloud\\nlangchain.llms.octoai_endpoint\\nlangchain.llms.ollama\\nlangchain.llms.opaqueprompts\\nlangchain.llms.openai\\nlangchain.llms.openllm\\nlangchain.llms.openlm\\nlangchain.llms.petals\\nlangchain.llms.pipelineai\\nlangchain.llms.predibase\\nlangchain.llms.predictionguard\\nlangchain.llms.promptlayer_openai\\nlangchain.llms.replicate\\nlangchain.llms.rwkv\\nlangchain.llms.sagemaker_endpoint\\nlangchain.llms.self_hosted\\nlangchain.llms.self_hosted_hugging_face\\nlangchain.llms.stochasticai\\nlangchain.llms.symblai_nebula\\nlangchain.llms.textgen\\nlangchain.llms.titan_takeoff\\nlangchain.llms.tongyi\\nlangchain.llms.utils\\nlangchain.llms.vertexai\\nlangchain.llms.vllm\\nlangchain.llms.writer\\nlangchain.llms.xinference\\nlangchain.load.dump\\nlangchain.load.load\\nlangchain.load.serializable\\nlangchain.memory.buffer\\nlangchain.memory.buffer_window\\nlangchain.memory.chat_memory\\nlangchain.memory.chat_message_histories.cassandra\\nlangchain.memory.chat_message_histories.cosmos_db\\nlangchain.memory.chat_message_histories.dynamodb\\nlangchain.memory.chat_message_histories.file\\nlangchain.memory.chat_message_histories.firestore\\nlangchain.memory.chat_message_histories.in_memory\\nlangchain.memory.chat_message_histories.momento\\nlangchain.memory.chat_message_histories.mongodb\\nlangchain.memory.chat_message_histories.postgres\\nlangchain.memory.chat_message_histories',\n",
       " '.redis\\nlangchain.memory.chat_message_histories.rocksetdb\\nlangchain.memory.chat_message_histories.sql\\nlangchain.memory.chat_message_histories.streamlit\\nlangchain.memory.chat_message_histories.xata\\nlangchain.memory.chat_message_histories.zep\\nlangchain.memory.combined\\nlangchain.memory.entity\\nlangchain.memory.kg\\nlangchain.memory.motorhead_memory\\nlangchain.memory.readonly\\nlangchain.memory.simple\\nlangchain.memory.summary\\nlangchain.memory.summary_buffer\\nlangchain.memory.token_buffer\\nlangchain.memory.utils\\nlangchain.memory.vectorstore\\nlangchain.memory.zep_memory\\nlangchain.model_laboratory\\nlangchain.output_parsers.boolean\\nlangchain.output_parsers.combining\\nlangchain.output_parsers.datetime\\nlangchain.output_parsers.enum\\nlangchain.output_parsers.fix\\nlangchain.output_parsers.json\\nlangchain.output_parsers.list\\nlangchain.output_parsers.loading\\nlangchain.output_parsers.openai_functions\\nlangchain.output_parsers.pydantic\\nlangchain.output_parsers.rail_parser\\nlangchain.output_parsers.regex\\nlangchain.output_parsers.regex_dict\\nlangchain.output_parsers.retry\\nlangchain.output_parsers.structured\\nlangchain.output_parsers.xml\\nlangchain.prompts.base\\nlangchain.prompts.chat\\nlangchain.prompts.example_selector.base\\nlangchain.prompts.example_selector.length_based\\nlangchain.prompts.example_selector.ngram_overlap\\nlangchain.prompts.example_selector.semantic_similarity\\nlangchain.prompts.few_shot\\nlangchain.prompts.few_shot_with_templates\\nlangchain.prompts.loading\\nlangchain.prompts.pipeline\\nlangchain.prompts.prompt\\nlangchain.retrievers.arxiv\\nlangchain.retrievers.azure_cognitive_search\\nlangchain.retrievers.bm25\\nlangchain.retrievers.chaindesk\\nlangchain.retrievers.chatgpt_plugin_retriever\\nlangchain.retrievers.contextual_compression\\nlangchain.retrievers.databerry\\nlangchain.retrievers.docarray\\nlangchain.retrievers.document_compressors.base\\nlangchain.retrievers.document_compressors.chain_extract\\nlangchain.retrievers.document_compressors.chain_filter\\nlangchain.retrievers.document_compressors.cohere_rerank\\nlangchain.retrievers.document_compressors.embeddings_filter\\nlangchain.retrievers.elastic_search_bm25\\nlangchain.retrievers.ensemble\\nlangchain.retrievers.google_cloud_enterprise_search\\nlangchain.retrievers.kay\\nlangchain.re',\n",
       " 'trievers.kendra\\nlangchain.retrievers.knn\\nlangchain.retrievers.llama_index\\nlangchain.retrievers.merger_retriever\\nlangchain.retrievers.metal\\nlangchain.retrievers.milvus\\nlangchain.retrievers.multi_query\\nlangchain.retrievers.multi_vector\\nlangchain.retrievers.parent_document_retriever\\nlangchain.retrievers.pinecone_hybrid_search\\nlangchain.retrievers.pubmed\\nlangchain.retrievers.re_phraser\\nlangchain.retrievers.remote_retriever\\nlangchain.retrievers.self_query.base\\nlangchain.retrievers.self_query.chroma\\nlangchain.retrievers.self_query.dashvector\\nlangchain.retrievers.self_query.deeplake\\nlangchain.retrievers.self_query.elasticsearch\\nlangchain.retrievers.self_query.milvus\\nlangchain.retrievers.self_query.myscale\\nlangchain.retrievers.self_query.opensearch\\nlangchain.retrievers.self_query.pinecone\\nlangchain.retrievers.self_query.qdrant\\nlangchain.retrievers.self_query.redis\\nlangchain.retrievers.self_query.supabase\\nlangchain.retrievers.self_query.timescalevector\\nlangchain.retrievers.self_query.vectara\\nlangchain.retrievers.self_query.weaviate\\nlangchain.retrievers.svm\\nlangchain.retrievers.tfidf\\nlangchain.retrievers.time_weighted_retriever\\nlangchain.retrievers.vespa_retriever\\nlangchain.retrievers.weaviate_hybrid_search\\nlangchain.retrievers.web_research\\nlangchain.retrievers.wikipedia\\nlangchain.retrievers.zep\\nlangchain.retrievers.zilliz\\nlangchain.runnables.openai_functions\\nlangchain.schema.agent\\nlangchain.schema.cache\\nlangchain.schema.chat\\nlangchain.schema.chat_history\\nlangchain.schema.document\\nlangchain.schema.embeddings\\nlangchain.schema.exceptions\\nlangchain.schema.language_model\\nlangchain.schema.memory\\nlangchain.schema.messages\\nlangchain.schema.output\\nlangchain.schema.output_parser\\nlangchain.schema.prompt\\nlangchain.schema.prompt_template\\nlangchain.schema.retriever\\nlangchain.schema.runnable.base\\nlangchain.schema.runnable.config\\nlangchain.schema.runnable.passthrough\\nlangchain.schema.runnable.retry\\nlangchain.schema.runnable.router\\nlangchain.schema.runnable.utils\\nlangchain.schema.storage\\nlangchain.schema.vectorstore\\nlangchain.smith',\n",
       " '.evaluation.config\\nlangchain.smith.evaluation.name_generation\\nlangchain.smith.evaluation.progress\\nlangchain.smith.evaluation.runner_utils\\nlangchain.smith.evaluation.string_run_evaluator\\nlangchain.storage.encoder_backed\\nlangchain.storage.exceptions\\nlangchain.storage.file_system\\nlangchain.storage.in_memory\\nlangchain.storage.redis\\nlangchain.text_splitter\\nlangchain.tools.ainetwork.app\\nlangchain.tools.ainetwork.base\\nlangchain.tools.ainetwork.owner\\nlangchain.tools.ainetwork.rule\\nlangchain.tools.ainetwork.transfer\\nlangchain.tools.ainetwork.utils\\nlangchain.tools.ainetwork.value\\nlangchain.tools.amadeus.base\\nlangchain.tools.amadeus.closest_airport\\nlangchain.tools.amadeus.flight_search\\nlangchain.tools.amadeus.utils\\nlangchain.tools.arxiv.tool\\nlangchain.tools.azure_cognitive_services.form_recognizer\\nlangchain.tools.azure_cognitive_services.image_analysis\\nlangchain.tools.azure_cognitive_services.speech2text\\nlangchain.tools.azure_cognitive_services.text2speech\\nlangchain.tools.azure_cognitive_services.utils\\nlangchain.tools.base\\nlangchain.tools.bing_search.tool\\nlangchain.tools.brave_search.tool\\nlangchain.tools.dataforseo_api_search.tool\\nlangchain.tools.ddg_search.tool\\nlangchain.tools.edenai.audio_speech_to_text\\nlangchain.tools.edenai.audio_text_to_speech\\nlangchain.tools.edenai.edenai_base_tool\\nlangchain.tools.edenai.image_explicitcontent\\nlangchain.tools.edenai.image_objectdetection\\nlangchain.tools.edenai.ocr_identityparser\\nlangchain.tools.edenai.ocr_invoiceparser\\nlangchain.tools.edenai.text_moderation\\nlangchain.tools.eleven_labs.models\\nlangchain.tools.eleven_labs.text2speech\\nlangchain.tools.file_management.copy\\nlangchain.tools.file_management.delete\\nlangchain.tools.file_management.file_search\\nlangchain.tools.file_management.list_dir\\nlangchain.tools.file_management.move\\nlangchain.tools.file_management.read\\nlangchain.tools.file_management.utils\\nlangchain.tools.file_management.write\\nlangchain.tools.github.tool\\nlangchain.tools.gitlab.tool\\nlangchain.tools.gmail.base\\nlangchain.tools.gmail.create_draft\\nlangchain.tools.gmail.get_message\\nlangchain.tools.gmail.get_thread\\nlangchain.tools.gmail.search\\nlangchain.tools.gmail.send_message\\nlangchain.tools.gmail.utils\\nlangchain.tools.golden_query.tool\\nlangchain.tools.google_places.tool',\n",
       " '\\nlangchain.tools.google_search.tool\\nlangchain.tools.google_serper.tool\\nlangchain.tools.graphql.tool\\nlangchain.tools.human.tool\\nlangchain.tools.ifttt\\nlangchain.tools.interaction.tool\\nlangchain.tools.jira.tool\\nlangchain.tools.json.tool\\nlangchain.tools.metaphor_search.tool\\nlangchain.tools.multion.create_session\\nlangchain.tools.multion.update_session\\nlangchain.tools.nuclia.tool\\nlangchain.tools.office365.base\\nlangchain.tools.office365.create_draft_message\\nlangchain.tools.office365.events_search\\nlangchain.tools.office365.messages_search\\nlangchain.tools.office365.send_event\\nlangchain.tools.office365.send_message\\nlangchain.tools.office365.utils\\nlangchain.tools.openapi.utils.api_models\\nlangchain.tools.openweathermap.tool\\nlangchain.tools.playwright.base\\nlangchain.tools.playwright.click\\nlangchain.tools.playwright.current_page\\nlangchain.tools.playwright.extract_hyperlinks\\nlangchain.tools.playwright.extract_text\\nlangchain.tools.playwright.get_elements\\nlangchain.tools.playwright.navigate\\nlangchain.tools.playwright.navigate_back\\nlangchain.tools.playwright.utils\\nlangchain.tools.plugin\\nlangchain.tools.powerbi.tool\\nlangchain.tools.pubmed.tool\\nlangchain.tools.python.tool\\nlangchain.tools.render\\nlangchain.tools.requests.tool\\nlangchain.tools.scenexplain.tool\\nlangchain.tools.searchapi.tool\\nlangchain.tools.searx_search.tool\\nlangchain.tools.shell.tool\\nlangchain.tools.sleep.tool\\nlangchain.tools.spark_sql.tool\\nlangchain.tools.sql_database.tool\\nlangchain.tools.steamship_image_generation.tool\\nlangchain.tools.steamship_image_generation.utils\\nlangchain.tools.vectorstore.tool\\nlangchain.tools.wikipedia.tool\\nlangchain.tools.wolfram_alpha.tool\\nlangchain.tools.yahoo_finance_news\\nlangchain.tools.youtube.search\\nlangchain.tools.zapier.tool\\nlangchain.utilities.alpha_vantage\\nlangchain.utilities.apify\\nlangchain.utilities.arxiv\\nlangchain.utilities.awslambda\\nlangchain.utilities.bash\\nlangchain.utilities.bibtex\\nlangchain.utilities.bing_search\\nlangchain.utilities.brave_search\\nlangchain.utilities.dalle_image_generator\\nlangchain.utilities.dataforseo_api_search\\nlangchain.utilities.duckduckgo_search\\nlangchain.utilities.github\\nlangchain.utilities.gitlab\\nlangchain.utilities.golden_query\\nlangchain.utilities.google_places_api\\nlangchain.utilities.google_search\\nlangchain.utilities.google_serper\\nlangchain.utilities.graphql\\nlangchain.utilities.jira\\nlangchain.utilities',\n",
       " '.max_compute\\nlangchain.utilities.metaphor_search\\nlangchain.utilities.opaqueprompts\\nlangchain.utilities.openapi\\nlangchain.utilities.openweathermap\\nlangchain.utilities.portkey\\nlangchain.utilities.powerbi\\nlangchain.utilities.pubmed\\nlangchain.utilities.python\\nlangchain.utilities.redis\\nlangchain.utilities.requests\\nlangchain.utilities.scenexplain\\nlangchain.utilities.searchapi\\nlangchain.utilities.searx_search\\nlangchain.utilities.serpapi\\nlangchain.utilities.spark_sql\\nlangchain.utilities.sql_database\\nlangchain.utilities.tensorflow_datasets\\nlangchain.utilities.twilio\\nlangchain.utilities.vertexai\\nlangchain.utilities.wikipedia\\nlangchain.utilities.wolfram_alpha\\nlangchain.utilities.zapier\\nlangchain.utils.aiter\\nlangchain.utils.env\\nlangchain.utils.formatting\\nlangchain.utils.html\\nlangchain.utils.input\\nlangchain.utils.iter\\nlangchain.utils.json_schema\\nlangchain.utils.loading\\nlangchain.utils.math\\nlangchain.utils.openai_functions\\nlangchain.utils.pydantic\\nlangchain.utils.strings\\nlangchain.utils.utils\\nlangchain.vectorstores.alibabacloud_opensearch\\nlangchain.vectorstores.analyticdb\\nlangchain.vectorstores.annoy\\nlangchain.vectorstores.atlas\\nlangchain.vectorstores.awadb\\nlangchain.vectorstores.azuresearch\\nlangchain.vectorstores.bageldb\\nlangchain.vectorstores.cassandra\\nlangchain.vectorstores.chroma\\nlangchain.vectorstores.clarifai\\nlangchain.vectorstores.clickhouse\\nlangchain.vectorstores.dashvector\\nlangchain.vectorstores.deeplake\\nlangchain.vectorstores.dingo\\nlangchain.vectorstores.docarray.base\\nlangchain.vectorstores.docarray.hnsw\\nlangchain.vectorstores.docarray.in_memory\\nlangchain.vectorstores.elastic_vector_search\\nlangchain.vectorstores.elasticsearch\\nlangchain.vectorstores.epsilla\\nlangchain.vectorstores.faiss\\nlangchain.vectorstores.hologres\\nlangchain.vectorstores.lancedb\\nlangchain.vectorstores.llm_rails\\nlangchain.vectorstores.marqo\\nlangchain.vectorstores.matching_engine\\nlangchain.vectorstores.meilisearch\\nlangchain.vectorstores.milvus\\nlangchain.vectorstores.mongodb_atlas\\nlangchain.vectorstores.myscale\\nlangchain.vectorstores.neo4j_vector\\nlangchain.vectorstores.nucliadb\\nlangchain.vectorstores.opensearch_vector_search\\nlangchain.vectorstores.pgembedding\\nlangchain.vectorstores.pgvector\\nlangchain.vectorstores.pinecone\\nlangchain.vectorstores.qdr',\n",
       " 'ant\\nlangchain.vectorstores.redis.base\\nlangchain.vectorstores.redis.filters\\nlangchain.vectorstores.redis.schema\\nlangchain.vectorstores.rocksetdb\\nlangchain.vectorstores.scann\\nlangchain.vectorstores.singlestoredb\\nlangchain.vectorstores.sklearn\\nlangchain.vectorstores.sqlitevss\\nlangchain.vectorstores.starrocks\\nlangchain.vectorstores.supabase\\nlangchain.vectorstores.tair\\nlangchain.vectorstores.tencentvectordb\\nlangchain.vectorstores.tigris\\nlangchain.vectorstores.timescalevector\\nlangchain.vectorstores.typesense\\nlangchain.vectorstores.usearch\\nlangchain.vectorstores.utils\\nlangchain.vectorstores.vald\\nlangchain.vectorstores.vearch\\nlangchain.vectorstores.vectara\\nlangchain.vectorstores.weaviate\\nlangchain.vectorstores.xata\\nlangchain.vectorstores.zep\\nlangchain.vectorstores.zilliz\\nlangchain_experimental.autonomous_agents.autogpt.agent\\nlangchain_experimental.autonomous_agents.autogpt.memory\\nlangchain_experimental.autonomous_agents.autogpt.output_parser\\nlangchain_experimental.autonomous_agents.autogpt.prompt\\nlangchain_experimental.autonomous_agents.autogpt.prompt_generator\\nlangchain_experimental.autonomous_agents.baby_agi.baby_agi\\nlangchain_experimental.autonomous_agents.baby_agi.task_creation\\nlangchain_experimental.autonomous_agents.baby_agi.task_execution\\nlangchain_experimental.autonomous_agents.baby_agi.task_prioritization\\nlangchain_experimental.autonomous_agents.hugginggpt.hugginggpt\\nlangchain_experimental.autonomous_agents.hugginggpt.repsonse_generator\\nlangchain_experimental.autonomous_agents.hugginggpt.task_executor\\nlangchain_experimental.autonomous_agents.hugginggpt.task_planner\\nlangchain_experimental.comprehend_moderation.amazon_comprehend_moderation\\nlangchain_experimental.comprehend_moderation.base_moderation\\nlangchain_experimental.comprehend_moderation.base_moderation_callbacks\\nlangchain_experimental.comprehend_moderation.base_moderation_config\\nlangchain_experimental.comprehend_moderation.base_moderation_exceptions\\nlangchain_experimental.comprehend_moderation.intent\\nlangchain_experimental.comprehend_moderation.pii\\nlangchain_experimental.comprehend_moderation.toxicity\\nlangchain_experimental.cpal.constants\\nlangchain_experimental.data_anonymizer.base\\nlangchain_experimental.data_anonymizer',\n",
       " '.deanonymizer_mapping\\nlangchain_experimental.data_anonymizer.faker_presidio_mapping\\nlangchain_experimental.fallacy_removal.base\\nlangchain_experimental.fallacy_removal.models\\nlangchain_experimental.generative_agents.generative_agent\\nlangchain_experimental.generative_agents.memory\\nlangchain_experimental.graph_transformers.diffbot\\nlangchain_experimental.llms.anthropic_functions\\nlangchain_experimental.llms.jsonformer_decoder\\nlangchain_experimental.llms.llamaapi\\nlangchain_experimental.llms.rellm_decoder\\nlangchain_experimental.pal_chain.base\\nlangchain_experimental.plan_and_execute.agent_executor\\nlangchain_experimental.plan_and_execute.executors.agent_executor\\nlangchain_experimental.plan_and_execute.executors.base\\nlangchain_experimental.plan_and_execute.planners.base\\nlangchain_experimental.plan_and_execute.planners.chat_planner\\nlangchain_experimental.plan_and_execute.schema\\nlangchain_experimental.prompt_injection_identifier.hugging_face_identifier\\nlangchain_experimental.retrievers.vector_sql_database\\nlangchain_experimental.smart_llm.base\\nlangchain_experimental.sql.base\\nlangchain_experimental.sql.vector_sql\\nlangchain_experimental.tabular_synthetic_data.base\\nlangchain_experimental.tabular_synthetic_data.openai\\nlangchain_experimental.tot.base\\nlangchain_experimental.tot.checker\\nlangchain_experimental.tot.controller\\nlangchain_experimental.tot.memory\\nlangchain_experimental.tot.prompts\\nlangchain_experimental.tot.thought\\nlangchain_experimental.tot.thought_generation\\npydantic.main\\n\\n\\n\\n\\n            Â© 2023, Harrison Chase.\\n          Last updated on Oct 01, 2023.\\n      \\n\\n\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(text_chunks.keys())\n",
    "text_chunks[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_text_chunks = open('text_chunks.pkl', 'wb') \n",
    "pickle.dump(text_chunks, file_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
